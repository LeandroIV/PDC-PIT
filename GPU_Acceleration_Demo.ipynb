{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Programming and Hardware Acceleration Demo\n",
    "\n",
    "This notebook demonstrates the performance difference between CPU and GPU for various computational tasks including:\n",
    "- Vector addition\n",
    "- Matrix multiplication\n",
    "- Neural network operations\n",
    "\n",
    "Make sure to enable GPU runtime for this notebook:\n",
    "- Click \"Runtime\" > \"Change runtime type\"\n",
    "- Select \"GPU\" under Hardware accelerator\n",
    "- Click \"Save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=== GPU AVAILABILITY CHECK ===\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"✅ GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"Memory reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"❌ No GPU available. Running on CPU only.\")\n",
    "    print(\"Note: To enable GPU, go to Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "\n",
    "def print_separator():\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print_separator()\n",
    "print(f\"DEVICE INFORMATION:\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Addition Benchmark\n",
    "\n",
    "This benchmark compares the performance of CPU vs GPU for vector addition operations with vectors of increasing sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def vector_addition_benchmark(sizes):\n",
    "    \"\"\"Benchmark vector addition on CPU and GPU.\"\"\"\n",
    "    print_separator()\n",
    "    print(\"VECTOR ADDITION BENCHMARK\")\n",
    "    print(\"This test adds two vectors of increasing sizes\")\n",
    "    print(\"and compares the time taken on CPU vs GPU.\")\n",
    "    print_separator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        size_str = f\"{size:,}\"\n",
    "        print(f\"Testing vector addition with size {size_str}...\")\n",
    "        \n",
    "        # Create random vectors\n",
    "        vector_a = torch.rand(size)\n",
    "        vector_b = torch.rand(size)\n",
    "        \n",
    "        # CPU benchmark\n",
    "        start_time = time.time()\n",
    "        result_cpu = vector_a + vector_b\n",
    "        cpu_time = time.time() - start_time\n",
    "        print(f\"  CPU time: {cpu_time:.6f} seconds\")\n",
    "        \n",
    "        # GPU benchmark (if available)\n",
    "        if device.type == \"cuda\":\n",
    "            # Move vectors to GPU\n",
    "            vector_a_gpu = vector_a.to(device)\n",
    "            vector_b_gpu = vector_b.to(device)\n",
    "            \n",
    "            # Warm-up run\n",
    "            _ = vector_a_gpu + vector_b_gpu\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed run\n",
    "            start_time = time.time()\n",
    "            result_gpu = vector_a_gpu + vector_b_gpu\n",
    "            torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "            gpu_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"  GPU time: {gpu_time:.6f} seconds\")\n",
    "            print(f\"  Speedup: {cpu_time / gpu_time:.2f}x\")\n",
    "            \n",
    "            results.append((size, cpu_time, gpu_time, cpu_time / gpu_time))\n",
    "        else:\n",
    "            # If no GPU, just duplicate CPU time\n",
    "            results.append((size, cpu_time, cpu_time, 1.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run vector addition benchmark\n",
    "vector_sizes = [10_000_000, 50_000_000, 100_000_000]\n",
    "vector_results = vector_addition_benchmark(vector_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Benchmark\n",
    "\n",
    "This benchmark compares the performance of CPU vs GPU for matrix multiplication operations with matrices of increasing sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def matrix_multiplication_benchmark(sizes):\n",
    "    \"\"\"Benchmark matrix multiplication on CPU and GPU.\"\"\"\n",
    "    print_separator()\n",
    "    print(\"MATRIX MULTIPLICATION BENCHMARK\")\n",
    "    print(\"This test multiplies two matrices of increasing sizes\")\n",
    "    print(\"and compares the time taken on CPU vs GPU.\")\n",
    "    print_separator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"Testing matrix multiplication with size {size}x{size}...\")\n",
    "        \n",
    "        # Create random matrices\n",
    "        matrix_a = torch.rand(size, size)\n",
    "        matrix_b = torch.rand(size, size)\n",
    "        \n",
    "        # CPU benchmark\n",
    "        start_time = time.time()\n",
    "        result_cpu = torch.matmul(matrix_a, matrix_b)\n",
    "        cpu_time = time.time() - start_time\n",
    "        print(f\"  CPU time: {cpu_time:.6f} seconds\")\n",
    "        \n",
    "        # GPU benchmark (if available)\n",
    "        if device.type == \"cuda\":\n",
    "            # Move matrices to GPU\n",
    "            matrix_a_gpu = matrix_a.to(device)\n",
    "            matrix_b_gpu = matrix_b.to(device)\n",
    "            \n",
    "            # Warm-up run\n",
    "            _ = torch.matmul(matrix_a_gpu, matrix_b_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Timed run\n",
    "            start_time = time.time()\n",
    "            result_gpu = torch.matmul(matrix_a_gpu, matrix_b_gpu)\n",
    "            torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "            gpu_time = time.time() - start_time\n",
    "            \n",
    "            # Verify results match\n",
    "            result_from_gpu = result_gpu.cpu()\n",
    "            is_close = torch.allclose(result_cpu, result_from_gpu, rtol=1e-3, atol=1e-3)\n",
    "            \n",
    "            print(f\"  GPU time: {gpu_time:.6f} seconds\")\n",
    "            print(f\"  Speedup: {cpu_time / gpu_time:.2f}x\")\n",
    "            print(f\"  Results match: {is_close}\")\n",
    "            \n",
    "            results.append((size, cpu_time, gpu_time, cpu_time / gpu_time))\n",
    "        else:\n",
    "            # If no GPU, just duplicate CPU time\n",
    "            results.append((size, cpu_time, cpu_time, 1.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run matrix multiplication benchmark\n",
    "matrix_sizes = [1000, 2000, 4000]\n",
    "matrix_results = matrix_multiplication_benchmark(matrix_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Benchmark\n",
    "\n",
    "This benchmark compares the performance of CPU vs GPU for neural network forward and backward passes with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def neural_network_benchmark(batch_sizes):\n",
    "    \"\"\"Benchmark neural network forward and backward pass on CPU and GPU.\"\"\"\n",
    "    print_separator()\n",
    "    print(\"NEURAL NETWORK BENCHMARK\")\n",
    "    print(\"This test performs forward and backward passes through a neural network\")\n",
    "    print(\"with different batch sizes and compares CPU vs GPU performance.\")\n",
    "    print_separator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define a simple neural network\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(784, 256)\n",
    "            self.fc2 = torch.nn.Linear(256, 128)\n",
    "            self.fc3 = torch.nn.Linear(128, 10)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Testing neural network with batch size {batch_size}...\")\n",
    "        \n",
    "        # Create random input and target data\n",
    "        input_data = torch.rand(batch_size, 784)\n",
    "        target = torch.randint(0, 10, (batch_size,))\n",
    "        \n",
    "        # CPU benchmark\n",
    "        model_cpu = SimpleNN()\n",
    "        optimizer_cpu = torch.optim.SGD(model_cpu.parameters(), lr=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Forward pass\n",
    "        output_cpu = model_cpu(input_data)\n",
    "        loss_cpu = criterion(output_cpu, target)\n",
    "        # Backward pass\n",
    "        optimizer_cpu.zero_grad()\n",
    "        loss_cpu.backward()\n",
    "        optimizer_cpu.step()\n",
    "        \n",
    "        cpu_time = time.time() - start_time\n",
    "        print(f\"  CPU time: {cpu_time:.6f} seconds\")\n",
    "        \n",
    "        # GPU benchmark (if available)\n",
    "        if device.type == \"cuda\":\n",
    "            model_gpu = SimpleNN().to(device)\n",
    "            optimizer_gpu = torch.optim.SGD(model_gpu.parameters(), lr=0.01)\n",
    "            input_data_gpu = input_data.to(device)\n",
    "            target_gpu = target.to(device)\n",
    "            \n",
    "            # Warm-up run\n",
    "            _ = model_gpu(input_data_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # Forward pass\n",
    "            output_gpu = model_gpu(input_data_gpu)\n",
    "            loss_gpu = criterion(output_gpu, target_gpu)\n",
    "            # Backward pass\n",
    "            optimizer_gpu.zero_grad()\n",
    "            loss_gpu.backward()\n",
    "            optimizer_gpu.step()\n",
    "            torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "            \n",
    "            gpu_time = time.time() - start_time\n",
    "            print(f\"  GPU time: {gpu_time:.6f} seconds\")\n",
    "            print(f\"  Speedup: {cpu_time / gpu_time:.2f}x\")\n",
    "            \n",
    "            results.append((batch_size, cpu_time, gpu_time, cpu_time / gpu_time))\n",
    "        else:\n",
    "            # If no GPU, just duplicate CPU time\n",
    "            results.append((batch_size, cpu_time, cpu_time, 1.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run neural network benchmark\n",
    "batch_sizes = [64, 128, 256, 512, 1024]\n",
    "nn_results = neural_network_benchmark(batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Let's summarize the results of our benchmarks in tables and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def print_summary_table(title, headers, rows):\n",
    "    \"\"\"Print a nicely formatted table of results.\"\"\"\n",
    "    print_separator()\n",
    "    print(title)\n",
    "    print_separator()\n",
    "    \n",
    "    # Calculate column widths\n",
    "    col_widths = [max(len(str(row[i])) for row in [headers] + rows) + 2 for i in range(len(headers))]\n",
    "    \n",
    "    # Print header\n",
    "    header_row = \"\".join(str(headers[i]).ljust(col_widths[i]) for i in range(len(headers)))\n",
    "    print(header_row)\n",
    "    print(\"-\" * sum(col_widths))\n",
    "    \n",
    "    # Print rows\n",
    "    for row in rows:\n",
    "        formatted_row = []\n",
    "        for i, item in enumerate(row):\n",
    "            if isinstance(item, float):\n",
    "                formatted_row.append(f\"{item:.6f}\".ljust(col_widths[i]))\n",
    "            else:\n",
    "                formatted_row.append(str(item).ljust(col_widths[i]))\n",
    "        print(\"\".join(formatted_row))\n",
    "\n",
    "# Format and print vector addition results\n",
    "formatted_vector_results = []\n",
    "for size, cpu_time, gpu_time, speedup in vector_results:\n",
    "    formatted_vector_results.append((f\"{size:,}\", cpu_time, gpu_time, speedup))\n",
    "\n",
    "print_summary_table(\n",
    "    \"VECTOR ADDITION SUMMARY\",\n",
    "    [\"Vector Size\", \"CPU Time (s)\", \"GPU Time (s)\", \"Speedup\"],\n",
    "    formatted_vector_results\n",
    ")\n",
    "\n",
    "# Format and print matrix multiplication results\n",
    "formatted_matrix_results = []\n",
    "for size, cpu_time, gpu_time, speedup in matrix_results:\n",
    "    formatted_matrix_results.append((f\"{size}x{size}\", cpu_time, gpu_time, speedup))\n",
    "\n",
    "print_summary_table(\n",
    "    \"MATRIX MULTIPLICATION SUMMARY\",\n",
    "    [\"Matrix Size\", \"CPU Time (s)\", \"GPU Time (s)\", \"Speedup\"],\n",
    "    formatted_matrix_results\n",
    ")\n",
    "\n",
    "# Format and print neural network results\n",
    "formatted_nn_results = []\n",
    "for batch_size, cpu_time, gpu_time, speedup in nn_results:\n",
    "    formatted_nn_results.append((batch_size, cpu_time, gpu_time, speedup))\n",
    "\n",
    "print_summary_table(\n",
    "    \"NEURAL NETWORK SUMMARY\",\n",
    "    [\"Batch Size\", \"CPU Time (s)\", \"GPU Time (s)\", \"Speedup\"],\n",
    "    formatted_nn_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "Let's create visualizations to better understand the performance differences between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_benchmark_results(title, x_label, sizes, cpu_times, gpu_times, speedups):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar positions\n",
    "    bar_width = 0.35\n",
    "    r1 = np.arange(len(sizes))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(r1, cpu_times, color='blue', width=bar_width, label='CPU')\n",
    "    plt.bar(r2, gpu_times, color='green', width=bar_width, label='GPU')\n",
    "    \n",
    "    # Add speedup as a line\n",
    "    plt.plot([x + bar_width/2 for x in r1], speedups, 'ro-', label='Speedup')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title(title)\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(sizes))], [str(size) for size in sizes])\n",
    "    \n",
    "    # Add speedup values as text\n",
    "    for i, speedup in enumerate(speedups):\n",
    "        plt.text(i + bar_width/2, max(cpu_times[i], gpu_times[i]) + 0.05, f'{speedup:.1f}x', \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Only create plots if GPU is available\n",
    "if device.type == \"cuda\":\n",
    "    # Vector addition plot\n",
    "    plot_benchmark_results(\n",
    "        \"Vector Addition: CPU vs GPU Performance\",\n",
    "        \"Vector Size\",\n",
    "        [result[0] for result in vector_results],\n",
    "        [result[1] for result in vector_results],\n",
    "        [result[2] for result in vector_results],\n",
    "        [result[3] for result in vector_results]\n",
    "    )\n",
    "    \n",
    "    # Matrix multiplication plot\n",
    "    plot_benchmark_results(\n",
    "        \"Matrix Multiplication: CPU vs GPU Performance\",\n",
    "        \"Matrix Size\",\n",
    "        [result[0] for result in matrix_results],\n",
    "        [result[1] for result in matrix_results],\n",
    "        [result[2] for result in matrix_results],\n",
    "        [result[3] for result in matrix_results]\n",
    "    )\n",
    "    \n",
    "    # Neural network plot\n",
    "    plot_benchmark_results(\n",
    "        \"Neural Network: CPU vs GPU Performance\",\n",
    "        \"Batch Size\",\n",
    "        [result[0] for result in nn_results],\n",
    "        [result[1] for result in nn_results],\n",
    "        [result[2] for result in nn_results],\n",
    "        [result[3] for result in nn_results]\n",
    "    )\n",
    "else:\n",
    "    print(\"No GPU available for visualization. Enable GPU to see performance comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let's summarize our findings from the GPU acceleration benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_separator()\n",
    "print(\"BENCHMARK CONCLUSION\")\n",
    "print_separator()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # Calculate average speedups\n",
    "    vector_speedup = sum(s for _, _, _, s in vector_results) / len(vector_results)\n",
    "    matrix_speedup = sum(s for _, _, _, s in matrix_results) / len(matrix_results)\n",
    "    nn_speedup = sum(s for _, _, _, s in nn_results) / len(nn_results)\n",
    "    \n",
    "    print(f\"GPU acceleration provided significant performance improvements:\")\n",
    "    print(f\"  • Vector Addition: {vector_speedup:.2f}x average speedup\")\n",
    "    print(f\"  • Matrix Multiplication: {matrix_speedup:.2f}x average speedup\")\n",
    "    print(f\"  • Neural Network Operations: {nn_speedup:.2f}x average speedup\")\n",
    "    print(\"\\nKey observations:\")\n",
    "    print(\"  • Matrix operations show the greatest benefit from GPU acceleration\")\n",
    "    print(\"  • Larger data sizes generally show better GPU utilization\")\n",
    "    print(\"  • Neural network training is significantly faster on GPU\")\n",
    "    print(\"\\nThis demonstrates why GPUs are essential for:\")\n",
    "    print(\"  • Deep learning and AI applications\")\n",
    "    print(\"  • Scientific computing and simulations\")\n",
    "    print(\"  • Data processing at scale\")\n",
    "else:\n",
    "    print(\"No GPU was available for this benchmark.\")\n",
    "    print(\"To see the benefits of GPU acceleration:\")\n",
    "    print(\"  1. Go to Runtime > Change runtime type\")\n",
    "    print(\"  2. Select 'GPU' under Hardware accelerator\")\n",
    "    print(\"  3. Click 'Save' and run this notebook again\")\n",
    "    print(\"\\nWith a GPU, you would typically see:\")\n",
    "    print(\"  • 10-50x speedup for matrix multiplication\")\n",
    "    print(\"  • 5-20x speedup for vector operations\")\n",
    "    print(\"  • 10-100x speedup for neural network training\")\n",
    "\n",
    "print_separator()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
